Porig.sens = harmonic_p( pw.PorigSens ),
# overall proportion (within this experiment) expected to agree
# @@insert actual sig agreement
SigAgree = n_perc_string( repSignif == origSignif & repDirection == origDirection),
PercSigAgree1 = paste( round( 100 * mean(pw.PsigAgree1), 0 ), "%", sep ="" )
)
View(expTable)
#bm
# quick look at results
# stringsWith( pattern = "pw", x = names(dat) )
#
takeMean = c("pw.PIRepInside",  # function:
"pw.PIRepInside.sens",
"pw.Porig",
"pw.PorigSens",
"pw.ratio",
"pw.PsigAgree1",
"pw.FEest")
# this is broken:
# res = dat %>% select(takeMean) %>%
#   mutate( across( .cols = everything(),
#                   .fns = mean) )
#bm
# quick look at results
# stringsWith( pattern = "pw", x = names(dat) )
#
takeMean = c("pw.PIRepInside",  # function:
"pw.PIRepInside.sens",
"pw.Porig",
"pw.PorigSens",
"pw.ratio",
"pw.diff",
"pw.PsigAgree1",
"pw.FEest")
# this is broken:
# res = dat %>% select(takeMean) %>%
#   mutate( across( .cols = everything(),
#                   .fns = mean) )
# x: vector of 0/1s
# @@note that this removes NAs
n_perc_string = function(x, digits = 0) {
if ( all( is.na(x) ) ) return("All missing")
x = x[!is.na(x)]
paste( sum(x), " (", round( 100 * mean(x), digits ), "%)", sep = "" )
}
n_perc_string( c(0,0,0,0,1,1,1,0,0) )
harmonic_p = function(x) {
library(harmonicmeanp)
if ( all( is.na(x) ) ) return(NA)
# @@note: better use p.hmp here becuase it's asymptotically exact,
#  but it was giving error messages
hmp.stat( x[ !is.na(x) ] )
}
#bm: instead of doing strings, should use numbers throughout and then post-process
#  that way we can use these in the plots
RE_string = function(yi, vi, digits = 2) {
mod = rma.uni(yi = yi,
vi = vi,
method = "REML")
paste( round( mod$b, digits ),
}
setwd(results.dir)
setwd("Tables to prettify")
write.csv(modTable, "moderator_regressions.csv")
# TABLE of these metrics at the experiment level (~50 rows)
#bm
# quick look at results
# stringsWith( pattern = "pw", x = names(dat) )
#
takeMean = c("pw.PIRepInside",  # function:
"pw.PIRepInside.sens",
"pw.Porig",
"pw.PorigSens",
"pw.ratio",
"pw.diff",
"pw.PsigAgree1",
"pw.FEest")
# this is broken:
# res = dat %>% select(takeMean) %>%
#   mutate( across( .cols = everything(),
#                   .fns = mean) )
# x: vector of 0/1s
# @@note that this removes NAs
n_perc_string = function(x, digits = 0) {
if ( all( is.na(x) ) ) return("All missing")
x = x[!is.na(x)]
paste( sum(x), " (", round( 100 * mean(x), digits ), "%)", sep = "" )
}
n_perc_string( c(0,0,0,0,1,1,1,0,0) )
harmonic_p = function(x) {
library(harmonicmeanp)
if ( all( is.na(x) ) ) return(NA)
# @@note: better use p.hmp here becuase it's asymptotically exact,
#  but it was giving error messages
hmp.stat( x[ !is.na(x) ] )
}
#bm: instead of doing strings, should use numbers throughout and then post-process
#  that way we can use these in the plots
RE_string = function(yi, vi, digits = 2) {
mod = rma.uni(yi = yi,
vi = vi,
method = "REML")
paste( round( mod$b, digits ),
}
# aggregation fns:
# plain mean (ratio)
# count and percent (repinside, PsigAGree)
# FE analysis (origES2, repES2, FEest)
# harmonic mean p-value (porig)
expTable = dat %>% group_by(peID) %>%
summarise( nOutcomes = n(),
FEEst
Ratio = round( mean(pw.ratio), 2 ),
PIRepInside = n_perc_string(pw.PIRepInside),
PIRepInside.sens = n_perc_string(pw.PIRepInside.sens),
# @@note: better use p.hmp here becuase it's asymptotically exact,
#  but it was giving error messages
Porig = harmonic_p( pw.Porig ),
Porig.sens = harmonic_p( pw.PorigSens ),
# overall proportion (within this experiment) expected to agree
# @@insert actual sig agreement
SigAgree = n_perc_string( repSignif == origSignif & repDirection == origDirection),
PercSigAgree1 = paste( round( 100 * mean(pw.PsigAgree1), 0 ), "%", sep ="" )
)
View(expTable)
rm(list=ls())
f common, use TVW's two conversions
# hi: upper CI limit
# goal of the fn is to fill in logOR and varLogRR for all entries
#  the other cols that get added in the interim are just intermediate steps
logHR_to_logOR = function(logHR,
rareY = rep(FALSE, length(logHR)),
lo = NA,
hi = NA){
#browser()
# # test only
# logHR = c(NA, NA, log(1.03), log(2), log(2))
# rareY = c(NA, NA, FALSE, TRUE, FALSE)
# lo = c(NA, NA, log(.8), NA, log(1.5))
# hi = c(NA, NA, NA, log(2.5), NA)
d = data.frame( logHR,
lo,
hi,
rareY)
##### Rare Outcome #####
# if outcome is rare, no transformation needed
d[ eqNA(d$rareY == TRUE), c("logOR", "loLogOR", "hiLogOR") ] = d[ eqNA(d$rareY == TRUE), c("logHR", "lo", "hi") ]
##### Common Outcome #####
# if outcome is common, use TVW's two transformations
# logHR ->(Biometrics Thm2 conversion) logOR ->(sqrt conversion) logRR
logHR_to_logRR_common = Vectorize( function(logHR){
logRR = rep(NA, length(logHR))
logRR[ !is.na(logHR) ] = log( ( 1 - 0.5^sqrt( exp(logHR[ !is.na(logHR) ]) ) ) / ( 1 - 0.5^sqrt( 1 / exp(logHR[ !is.na(logHR) ]) ) ) )
return(logRR)
} )
#logHR_to_logRR_common( c(log(1.4), log(.745), NA) )
logRR_to_logOR_common = Vectorize( function(logRR){
logOR = rep(NA, length(logRR))
logOR[ !is.na(logRR) ] = log( sqrt( exp( logRR[ !is.na(logRR) ] ) ) )
return(logOR)
} )
#logRR_to_logOR_common( c(log(1.4), log(.745), NA) )
# first convert logHR -> logRR via
#  TVW's Biometrics conversion (Thm 2)
d[ eqNA(d$rareY == FALSE), c("logRR", "loLogRR", "hiLogRR") ] = logHR_to_logRR_common( d[ eqNA(d$rareY == FALSE), c("logHR", "lo", "hi") ] )
# now convert the RRs to ORs via square-root
#     @bm: this is hitting an error
d[ eqNA(d$rareY == FALSE), c("logOR", "loLogOR", "hiLogOR") ] = logRR_to_logOR_common( d[ eqNA(d$rareY == FALSE), c("logRR", "loLogRR", "hiLogRR") ] )
##### Get Variance from CI #####
# use either lower or upper limit, depending on what's available
d$lim = d$loLogOR
d$lim[ is.na(d$loLogOR) ] = d$hiLogOR[ is.na(d$loLogOR) ]
d$varLogOR[ !is.na(d$lim) ] = ci_to_var( est = d$logOR[ !is.na(d$lim) ],
ci.lim = d$lim[ !is.na(d$lim) ] )
return(d)
}
# sanity checks:
# test only
logHR = c(NA, NA, log(1.03), log(2), log(2))
rareY = c(NA, NA, FALSE, TRUE, FALSE)
lo = c(NA, NA, log(.8), NA, log(1.5))
hi = c(NA, NA, NA, log(2.5), NA)
res = logHR_to_logOR( logHR, rareY, lo, hi )
source('~/Dropbox/Personal computer/Independent studies/2020/RPCB reproducibility cancer biology/Code (git)/helper.R')
# ALSO PUT IN METAUTILITY
# tests proposition x
# but returns FALSE instead of NA if x itself is NA
eqNA = function(x){
!is.na(x) & x == 1
}
eqNA(NA == 5)
hi: upper CI limit
# goal of the fn is to fill in logOR and varLogRR for all entries
#  the other cols that get added in the interim are just intermediate steps
logHR_to_logOR = function(logHR,
rareY = rep(FALSE, length(logHR)),
lo = NA,
hi = NA){
#browser()
# # test only
# logHR = c(NA, NA, log(1.03), log(2), log(2))
# rareY = c(NA, NA, FALSE, TRUE, FALSE)
# lo = c(NA, NA, log(.8), NA, log(1.5))
# hi = c(NA, NA, NA, log(2.5), NA)
d = data.frame( logHR,
lo,
hi,
rareY)
##### Rare Outcome #####
# if outcome is rare, no transformation needed
d[ eqNA(d$rareY == TRUE), c("logOR", "loLogOR", "hiLogOR") ] = d[ eqNA(d$rareY == TRUE), c("logHR", "lo", "hi") ]
##### Common Outcome #####
# if outcome is common, use TVW's two transformations
# logHR ->(Biometrics Thm2 conversion) logOR ->(sqrt conversion) logRR
logHR_to_logRR_common = Vectorize( function(logHR){
logRR = rep(NA, length(logHR))
logRR[ !is.na(logHR) ] = log( ( 1 - 0.5^sqrt( exp(logHR[ !is.na(logHR) ]) ) ) / ( 1 - 0.5^sqrt( 1 / exp(logHR[ !is.na(logHR) ]) ) ) )
return(logRR)
} )
#logHR_to_logRR_common( c(log(1.4), log(.745), NA) )
logRR_to_logOR_common = Vectorize( function(logRR){
logOR = rep(NA, length(logRR))
logOR[ !is.na(logRR) ] = log( sqrt( exp( logRR[ !is.na(logRR) ] ) ) )
return(logOR)
} )
#logRR_to_logOR_common( c(log(1.4), log(.745), NA) )
# first convert logHR -> logRR via
#  TVW's Biometrics conversion (Thm 2)
d[ eqNA(d$rareY == FALSE), c("logRR", "loLogRR", "hiLogRR") ] = logHR_to_logRR_common( d[ eqNA(d$rareY == FALSE), c("logHR", "lo", "hi") ] )
# now convert the RRs to ORs via square-root
#     @bm: this is hitting an error
d[ eqNA(d$rareY == FALSE), c("logOR", "loLogOR", "hiLogOR") ] = logRR_to_logOR_common( d[ eqNA(d$rareY == FALSE), c("logRR", "loLogRR", "hiLogRR") ] )
##### Get Variance from CI #####
# use either lower or upper limit, depending on what's available
d$lim = d$loLogOR
d$lim[ is.na(d$loLogOR) ] = d$hiLogOR[ is.na(d$loLogOR) ]
d$varLogOR[ !is.na(d$lim) ] = ci_to_var( est = d$logOR[ !is.na(d$lim) ],
ci.lim = d$lim[ !is.na(d$lim) ] )
return(d)
}
# sanity checks:
# test only
logHR = c(NA, NA, log(1.03), log(2), log(2))
rareY = c(NA, NA, FALSE, TRUE, FALSE)
lo = c(NA, NA, log(.8), NA, log(1.5))
hi = c(NA, NA, NA, log(2.5), NA)
res = logHR_to_logOR( logHR, rareY, lo, hi )
source('~/Dropbox/Personal computer/Independent studies/2020/RPCB reproducibility cancer biology/Code (git)/helper.R')
# sanity checks:
# test only
logHR = c(NA, NA, log(1.03), log(2), log(2))
rareY = c(NA, NA, FALSE, TRUE, FALSE)
lo = c(NA, NA, log(.8), NA, log(1.5))
hi = c(NA, NA, NA, log(2.5), NA)
res = logHR_to_logOR( logHR, rareY, lo, hi )
# rare
expect_equal( res$logHR[4], res$logOR[4] )
expect_equal( ( (res$hi[4] - res$logHR[4]) / qnorm(.975) )^2, res$varLogOR[4] )
# common
term = 0.5^sqrt( exp(res$logHR[3]) )
term2 = 0.5^sqrt( 1/exp(res$logHR[3]) )
expect_equal( res$logRR[3], log( (1 - term) / (1 - term2) ) )  # check RR
expect_equal( res$logOR[3], log( sqrt( exp( res$logRR[3] ) ) ) )  # check OR
term = 0.5^sqrt( exp(res$lo[3]) )
term2 = 0.5^sqrt( 1/exp(res$lo[3]) )
expect_equal( res$loLogRR[3], log( (1 - term) / (1 - term2) ) )  # check RR limit
expect_equal( res$loLogOR[3], log( sqrt( exp( res$loLogRR[3] ) ) ) )  # check OR limit
expect_equal( ( (res$logOR[3] - res$loLogOR[3]) / qnorm(.975) )^2, res$varLogOR[3] )
# Hasselblad & Hedges conversion that we think only works for common outcomes
logOR_to_SMD = function(logOR,
lo = NA,
hi = NA){
d = data.frame( logOR,
lo,
hi)
# purpose of the internal fn here is to handle NAs in the above dataframe
.logOR_to_SMD = Vectorize( function(logOR){
SMD = rep(NA, length(logOR))
SMD[ !is.na(logOR) ] = ( sqrt(3) / pi ) * logOR[ !is.na(logOR) ]
return(SMD)
} )
d[ c("SMD", "loSMD", "hiSMD") ] = .logOR_to_SMD( d[ c("logOR", "lo", "hi") ] )
##### Get Variance from CI #####
# use either lower or upper limit, depending on what's available
d$lim = d$loSMD
d$lim[ is.na(d$loSMD) ] = d$hiSMD[ is.na(d$loSMD) ]
d$varSMD[ !is.na(d$lim) ] = ci_to_var( est = d$SMD[ !is.na(d$lim) ],
ci.lim = d$lim[ !is.na(d$lim) ] )
return(d)
}
# sanity check
logOR = c(log(1.03), log(.75), NA)
lo = c(log(.8), NA, NA)
hi = c(NA, log(.9), NA)
res = logOR_to_SMD( logOR, lo, hi )
expect_equal( res$SMD[1], sqrt(3)/pi * res$logOR[1] )
expect_equal( res$loSMD[1], sqrt(3)/pi * res$lo[1] )
# instead of what the fn is doing (i.e., convert CI limit itself), try getting var of
#  OR first from its CI limit
varLogOR = ( ( res$logOR[1] - res$lo[1] ) / qnorm(.975) )^2
expect_equal( res$varSMD[1], sqrt(3)/pi * varLogOR )
# @ WHY SO DIFFERENT?
table(d2$EStype)
# read back in
d2 = read_interm("intermediate_dataset_step2.csv")
##### Sanity checks on effect types #####
# breakdown of ES types
# sanity check:
# Tim said "For awareness, we are at 90 Cohen's d, 15 Cohen's dz, 20 Glass' delta, 6 Pearson's r, and 7 Hazard ratios for effects where there are quantitative pairs (this is counting them as unique paper/experiment/effect/internal replication, so naturally this goes down as they are collapsed on any of those elements for analysis/visualization)."
# that totals 138
d2 %>% filter( quantPair == TRUE ) %>%
group_by(EStype) %>%
summarise(n())
# matches :)
# look at which statistical tests yielded which effect size types
t = d2 %>% group_by(EStype, Statistical.test.applied.to.original.data) %>%
summarise(n())
##### ES2: Converted to a scale that can be meta-analyzed, but NOT necessarily SMDs #####
# statistics that should be converted to ES2 scale
toConvert = c("origES", "origESLo", "origESHi", "repES", "repESLo", "repESHi")
# convert column-by-column
for (i in toConvert) {
newName = paste(i, "2", sep="")
temp = convert_to_ES2( d2[[i]], .EStype = d2$EStype )
d2[[newName]] = temp$ES2
# this will be the same for each i, so just do it once
if (i == toConvert[1]) d2$ES2type = temp$ES2type
}
# sanity checks
data.frame( d2 %>% group_by(EStype, ES2type) %>%
summarise( meanNA(origES),
meanNA(origES2),
meanNA(repES),
meanNA(repES2) ) )
table(d2$ES2type)
##### Standard errors #####
# @@COULD ASK TIM FOR THE ORIGINAL SES IF AVAILABLE; THIS IS A TEMPORARY Z APPROXIMATION:
# 1.96 * SE * 2 = full CI width
d2$origSE2 = ( d2$origESHi2 - d2$origESLo2 ) / ( 2 * qnorm(.975) )
d2$repSE2 = ( d2$repESHi2 - d2$repESLo2 ) / ( 2 * qnorm(.975) )
d2$origVar2 = d2$origSE2^2
d2$repVar2 = d2$repSE2^2
##### ES3: Convert all to approximate SMDs #####
toConvert = c("origES2", "origESLo2", "origESHi2", "repES2", "repESLo2", "repESHi2")
library(stringr)
# convert column-by-column
for (i in toConvert) {
# name columns with "3", e.g., "origES3", etc.
newName = str_replace(string = i,
pattern = "2",
replacement = "3")
temp = convert_to_ES3( d2[[i]], .ES2type = d2$ES2type )
d2[[newName]] = temp$ES3
}
##### Standard errors #####
d2$origSE3 = ( d2$origESHi3 - d2$origESLo3 ) / ( 2 * qnorm(.975) )
d2$repSE3 = ( d2$repESHi3 - d2$repESLo3 ) / ( 2 * qnorm(.975) )
d2$origVar3 = d2$origSE3^2
d2$repVar3 = d2$repSE3^2
# read back in
d2 = read_interm("intermediate_dataset_step2.csv")
rm(list=ls())
library(readxl)
library(dplyr)
library(ggplot2)
library(MetaUtility)
library(robumeta)
library(testthat)
library(data.table)
library(tableone)
library(qdapTools)
library(metafor)
library(fastDummies)
root.dir = "~/Dropbox/Personal computer/Independent studies/2020/RPCB reproducibility cancer biology"
raw.data.dir = paste(root.dir, "Raw data", sep="/")
prepped.data.dir = paste(root.dir, "Prepped data", sep="/")
code.dir = paste(root.dir, "Code (git)", sep="/")
setwd(code.dir)
source("helper.R")
# should View2() open tabs?
useView = FALSE
# should we run sanity checks?
run.sanity = FALSE
# read in paper-, experiment-, and outcome-level data
setwd(raw.data.dir)
# we won't actually be using the first of these
dp = read_xlsx("2020_11_30_raw_data.xlsx", sheet = "Paper level data"); nrow(dp)
de = read_xlsx("2020_11_30_raw_data.xlsx", sheet = "Experiment level data"); nrow(de)
do = read_xlsx("2020_11_30_raw_data.xlsx", sheet = "Outcome level data"); nrow(do)
##### Sanity Checks on Hierarchical Data Structure #####
# nesting levels: paper > experiment > outcome
names(de)
# exp-level data have 193 unique paper-exp combos
uni( paste(de$`Paper #`, de$`Experiment #` ) )
# and 53 papers
uni(de$`Paper #`)
# outcome-level data have only 50 unique paper-exp combos
uni( paste(do$`Paper #`, do$`Experiment #` ) )
# and 23 papers
uni(do$`Paper #`)
# outcome-level only contains ones with quantitative effect sizes
table( is.na(do$`Original effect size`))
# confirm that outcome-level data have all papers from exp-level data for
#  which the replication was completed
expect_equal( uni(de$`Paper #`[de$`Replication experiment completed` == "Yes"]),
uni(do$`Paper #`) )
##### Look at What Info Is in Each Dataset #####
# variables that only appear in one or the other dataset
names(do)[ !names(do) %in% names(de) ]
names(de)[ !names(de) %in% names(do) ]  # moderators
# anything non-overlapping in paper-level data?
names(dp)[ !names(dp) %in% c(names(de), names(do)) ]
# variables in both datasets
names(do)[ names(do) %in% names(de) ]
names(dp)[ names(dp) %in% names(de) ]
##### Merge Datasets #####
d = merge( do, de, by = c("Paper #", "Experiment #"), all=TRUE )
nrow(d)
# because de has a superset of do's papers and experiments:
expect_equal( uni(d$`Paper #`), uni(de$`Paper #`) )
expect_equal( uni(d$`Experiment #`), uni(de$`Experiment #`) )
# this time don't keep papers that don't even give experiment-level data
# i.e., left-join
d = merge( d, dp, by = "Paper #", all.x = TRUE)
nrow(d)
# read back in
d2 = read_interm("intermediate_dataset_step2.csv")
##### Sanity checks on effect types #####
# breakdown of ES types
# sanity check:
# Tim said "For awareness, we are at 90 Cohen's d, 15 Cohen's dz, 20 Glass' delta, 6 Pearson's r, and 7 Hazard ratios for effects where there are quantitative pairs (this is counting them as unique paper/experiment/effect/internal replication, so naturally this goes down as they are collapsed on any of those elements for analysis/visualization)."
# that totals 138
d2 %>% filter( quantPair == TRUE ) %>%
group_by(EStype) %>%
summarise(n())
# matches :)
# look at which statistical tests yielded which effect size types
t = d2 %>% group_by(EStype, Statistical.test.applied.to.original.data) %>%
summarise(n())
##### ES2: Converted to a scale that can be meta-analyzed, but NOT necessarily SMDs #####
# statistics that should be converted to ES2 scale
toConvert = c("origES", "origESLo", "origESHi", "repES", "repESLo", "repESHi")
# convert column-by-column
for (i in toConvert) {
newName = paste(i, "2", sep="")
temp = convert_to_ES2( d2[[i]], .EStype = d2$EStype )
d2[[newName]] = temp$ES2
# this will be the same for each i, so just do it once
if (i == toConvert[1]) d2$ES2type = temp$ES2type
}
# sanity checks
data.frame( d2 %>% group_by(EStype, ES2type) %>%
summarise( meanNA(origES),
meanNA(origES2),
meanNA(repES),
meanNA(repES2) ) )
table(d2$ES2type)
##### Standard errors #####
# @@COULD ASK TIM FOR THE ORIGINAL SES IF AVAILABLE; THIS IS A TEMPORARY Z APPROXIMATION:
# 1.96 * SE * 2 = full CI width
d2$origSE2 = ( d2$origESHi2 - d2$origESLo2 ) / ( 2 * qnorm(.975) )
d2$repSE2 = ( d2$repESHi2 - d2$repESLo2 ) / ( 2 * qnorm(.975) )
d2$origVar2 = d2$origSE2^2
d2$repVar2 = d2$repSE2^2
##### ES3: Convert all to approximate SMDs #####
toConvert = c("origES2", "origESLo2", "origESHi2", "repES2", "repESLo2", "repESHi2")
library(stringr)
# convert column-by-column
for (i in toConvert) {
# name columns with "3", e.g., "origES3", etc.
newName = str_replace(string = i,
pattern = "2",
replacement = "3")
temp = convert_to_ES3( d2[[i]], .ES2type = d2$ES2type )
d2[[newName]] = temp$ES3
}
##### Standard errors #####
d2$origSE3 = ( d2$origESHi3 - d2$origESLo3 ) / ( 2 * qnorm(.975) )
d2$repSE3 = ( d2$repESHi3 - d2$repESLo3 ) / ( 2 * qnorm(.975) )
d2$origVar3 = d2$origSE3^2
d2$repVar3 = d2$repSE3^2
table(d2$EStype)
which( d2$EStype == "Cohen's d" )[1]
d2$origES3[ind]
ind = which( d2$EStype == "Cohen's d" )[1]
d2$origES3[ind]
ind = which( d2$EStype %in% c("Cohen's d",
"Cohen's dz",
"Glass' delta" )
)
ind
expect_equal( d2$origES3[ind], d2$origES[ind] )
expect_equal( d2$repES3[ind], d2$repES[ind] )
expect_equal( d2$origESLo3[ind], d2$origESLo[ind] )
##### Sanity checks #####
# check each type
table(d2$EStype)
( 1 − 0.5 * sqrt(d2$origES[ind]) ) / ( 1 − 0.5 * sqrt(1/d2$origES[ind]) )
( 1 - 0.5 * sqrt(d2$origES[ind]) ) / ( 1 - 0.5 * sqrt(1/d2$origES[ind]) )
myRR = ( 1 - 0.5 * sqrt(d2$origES[ind]) ) / ( 1 - 0.5 * sqrt(1/d2$origES[ind]) )
myRR = ( 1 - 0.5 * sqrt(d2$origES[ind]) ) / ( 1 - 0.5 * sqrt(1/d2$origES[ind]) )
myOR = sqrt(myRR)
myRR = ( 1 - 0.5 * sqrt(d2$origES[ind]) ) / ( 1 - 0.5 * sqrt(1/d2$origES[ind]) )
myOR = sqrt(myRR)
mySMD = 0.91 * log(myOR)
mySMD
expect_equal( d2$origES3[ind], mySMD )
# check all HRs (common-outcome conversions)
ind = which( d2$EStype == "Hazard ratio" )
myRR = ( 1 - 0.5 * sqrt(d2$origES[ind]) ) / ( 1 - 0.5 * sqrt(1/d2$origES[ind]) )
3.99/3.5
4.99/5
